{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Document Similarity using NLTK and gensim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tokenization, word counts, and possibly calculated tf-idf scores for words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization in NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import string\n",
    "\n",
    "from collections import Counter\n",
    "import urllib\n",
    "\n",
    "# nltk 코퍼스 파일 받아오고 열기\n",
    "text = nltk.corpus.gutenberg.raw(\"burgess-busterbrown.txt\")\n",
    "\n",
    "# raw text를 모두 소문자로 바꾸고 특수문자를 제거한 뒤 tokenize하기\n",
    "def get_tokens(text):\n",
    "    lowers = text.lower()\n",
    "    tokenizer = nltk.tokenize.RegexpTokenizer(r'\\w+')\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "\n",
    "    return tokens\n",
    "\n",
    "tokens = get_tokens(text)\n",
    "print(tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize된 문서 안에서 각 단어의 빈도 세기\n",
    "count = Counter(tokens)\n",
    "\n",
    "# 가장 많이 쓰인 단어부터 상위 10개 확인\n",
    "print(count.most_common(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nltk를 이용해 stopword 제거하기 \n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "tokens = get_tokens(text)\n",
    "filtered = [w for w in tokens if not w in stopwords.words('english')]\n",
    "\n",
    "# stopword가 제거된 토큰들의 빈도 세기\n",
    "count = Counter(filtered)\n",
    "\n",
    "# 가장 많이 쓰인 토큰부터 상위 100개 확인\n",
    "print(count.most_common(100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 각 토큰의 stem 얻기\n",
    "\n",
    "from nltk.stem.porter import *\n",
    "\n",
    "def stem_tokens(tokens, stemmer):\n",
    "    stemmed = []\n",
    "    for item in tokens:\n",
    "        stemmed.append(stemmer.stem(item))\n",
    "    return stemmed\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "stemmed = stem_tokens(filtered, stemmer)\n",
    "\n",
    "# stemmize된 토큰들의 빈도 세기\n",
    "count = Counter(stemmed)\n",
    "\n",
    "# 가장 많이 쓰인 stem부터 상위 100개 확인\n",
    "print(count.most_common(100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tf-Idf in gensim\n",
    "###### gensim 설치 명령: pip install --upgrade gensim\n",
    "###### gensim installation instructions: https://radimrehurek.com/gensim/install.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import string\n",
    "import gensim\n",
    "\n",
    "# 지금까지 했던 tokenize, stopword 제거, punctuation 제거, stemmize를 모두 수행하는 새 tokenize 함수\n",
    "def tokenize(text):\n",
    "    tokens = get_tokens(text)\n",
    "    stems = stem_tokens(tokens, stemmer)\n",
    "    return stems\n",
    "\n",
    "# nltk gutenberg 텍스트의 목록 불러오기\n",
    "titles = nltk.corpus.gutenberg.fileids()\n",
    "print(titles)\n",
    "\n",
    "# 목록에 있는 텍스트를 순서대로 tokenize(stemize)한 뒤 하나의 리스트로 모으기\n",
    "text_tokens = []\n",
    "for title in titles:\n",
    "    text = nltk.corpus.gutenberg.raw(title)\n",
    "    tokens = tokenize(text)\n",
    "    text_tokens.append(tokens)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 각 리스트에 들어 있는 tokenized text의 형태\n",
    "print(text_tokens[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gensim 패키지를 이용하여 단어들을 숫자 인덱스로 바꾸어 쓰기\n",
    "# 딕셔너리 형태로 저장\n",
    "\n",
    "from gensim import corpora\n",
    "dictionary = corpora.Dictionary(text_tokens)\n",
    "\n",
    "# 인덱스로 단어 호출하기 \n",
    "print(dictionary[10])\n",
    "print(dictionary[100])\n",
    "\n",
    "# 단어로 인덱스 호출하기 \n",
    "print(dictionary.token2id['road'])\n",
    "print(dictionary.token2id['like'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 딕셔너리 내 단어들의 총 개수 \n",
    "print(\"Number of words in dictionary:\",len(dictionary))\n",
    "\n",
    "# 각 인덱스가 부여된 단어들의 목록 \n",
    "for i in dictionary.keys():\n",
    "    print(i, dictionary[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 위에서 정의한 인덱스에 따라 각 텍스트를 bag of words의 형태로 바꾸어 코퍼스를 구성\n",
    "corpus = [dictionary.doc2bow(text) for text in text_tokens]\n",
    "\n",
    "print(dictionary.doc2bow(text_tokens[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a tf-idf model from the corpus\n",
    "tf_idf = gensim.models.TfidfModel(corpus)\n",
    "print(tf_idf)\n",
    "\n",
    "# tf-idf 모델에 따라 문서에 사용된 단어들의 값이 산출됨\n",
    "tf_idf[corpus[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a similarity measure object in tf-idf space\n",
    "sims = gensim.similarities.Similarity('', tf_idf[corpus], num_features=len(dictionary))\n",
    "print(type(sims))\n",
    "\n",
    "# Each shard is stored to disk under output_prefix.shard_number.\n",
    "# If you don’t specify an output prefix, a random filename in temp will be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sims[tf_idf[corpus[4]]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a query document and convert it to tf-idf\n",
    "\n",
    "# query document을 위의 방법에 따라 tokenize 하기\n",
    "query_doc = [w.lower() for w in tokenize(\"Emma Woodhouse, handsome, clever, and rich, with a comfortable home and happy disposition, seemed to unite some of the best blessings of existence.\")]\n",
    "print(query_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# query document를 위 코퍼스의 인덱스에 따라 bag of words로 변환\n",
    "query_doc_bow = dictionary.doc2bow(query_doc)\n",
    "print(query_doc_bow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# query document를 위 tf-idf 모델에 따라 표현\n",
    "query_doc_tf_idf = tf_idf[query_doc_bow]\n",
    "print(query_doc_tf_idf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# document similarities to query\n",
    "print(sims[query_doc_tf_idf])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
