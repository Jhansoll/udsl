{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2Vec 사용하기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### gensim 패키지 이용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### gensim 설치 명령: pip install --upgrade gensim\n",
    "#### gensim installation instructions: https://radimrehurek.com/gensim/install.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Word2Vec 모델 학습시키기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델에 들어갈 training data 만들기\n",
    "sentences = [['once', 'upon', 'a', 'time', 'there', 'was', 'an', 'old', 'sow', 'with', 'three', 'little', 'pig'],\n",
    "             ['the', 'first', 'that', 'went', 'off', 'met', 'a', 'man', 'with', 'a', 'bundle', 'of', 'straw'],\n",
    "             ['the', 'second', 'pig', 'met', 'a', 'man', 'with', 'a', 'bundle', 'of', 'furze'],\n",
    "             ['the', 'third', 'little', 'pig', 'met', 'a', 'men', 'with', 'a', 'load', 'of', 'bricks']]\n",
    "\n",
    "# 모델 학습시키기\n",
    "model = Word2Vec(sentences, min_count=1) # default size = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습된 모델 확인하기\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델에 구축된 vocabulary 확인하기\n",
    "words = list(model.wv.vocab)\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 단어 하나에 대한 임베딩 벡터 얻기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model['pig'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model['little'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 별도의 파일로 모델 저장하기\n",
    "model.save('model.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 저장해 둔 모델 불러오기\n",
    "new_model = Word2Vec.load('model.bin')\n",
    "print(new_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 단어 임베딩 시각화하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCA를 이용하여 단어 벡터를 시각화\n",
    "# PCA는 고차원의 단어 벡터를 2차원으로 줄여 시각화할 수 있는 형태로 만들어 줌 \n",
    "from sklearn.decomposition import PCA\n",
    "from matplotlib import pyplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습된 word2vec 모델\n",
    "# vocabulary에 들어 있는 단어 각각에 대해 100차원의 벡터가 생성되어 있음\n",
    "X = model[model.wv.vocab]\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 이 벡터(여기서는 30x100)에 2차원 PCA 모델을 적용시켜 2차원으로 바꾸어 줌\n",
    "pca = PCA(n_components=2)\n",
    "result = pca.fit_transform(X)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2차원 프로젝션 결과를 산점도에 그릴 수 있음\n",
    "# 각 단어는 모델에서 학습한 의미에 따라 적절한 위치에 그려지게 됨\n",
    "pyplot.scatter(result[:, 0], result[:, 1])\n",
    "words = list(model.wv.vocab)\n",
    "for i, word in enumerate(words):\n",
    "    pyplot.annotate(word, xy=(result[i, 0], result[i, 1]))\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Google’s Word2Vec Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 미리 학습해 둔 word2vec 모형을 사용할 수 있음\n",
    "# 구글에서 뉴스 데이터를 가지고 학습, 구축한 것이 대표적\n",
    "# 300만 개 정도의 단어와 phrase로 구성, 300차원으로 표현\n",
    "\n",
    "# url:\n",
    "# https://drive.google.com/file/d/0B7XkCwpI5KDYNlNUTTlSS21pQmM/edit?usp=sharing\n",
    "\n",
    "path = '/Users/sana/Documents/'\n",
    "\n",
    "from gensim.models import KeyedVectors\n",
    "filename = path+'GoogleNews-vectors-negative300.bin'\n",
    "model = KeyedVectors.load_word2vec_format(filename, binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# word2vec analogy\n",
    "# calculate: (king - man) + woman = ?\n",
    "result = model.most_similar(positive=['woman', 'king'], negative=['man'], topn=1)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate: (Paris - France) + Korea = ?\n",
    "result2 = model.most_similar(positive=['Paris', 'Korea'], negative=['France'], topn=1)\n",
    "print(result2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Stanford's GloVe Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global Vectors for Word Representation\n",
    "# 스탠포드에서 만든 word embedding algorithm\n",
    "\n",
    "# pre-trained vectors:\n",
    "# http://ling.snu.ac.kr/class/cl_under1801/glove.6B.300d.txt.zip\n",
    "\n",
    "# 이것을 gensim을 통해 word2vec의 형식으로 바꾸어 사용 가능\n",
    "\n",
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "glove_input_file = path+'glove.6B.300d.txt'\n",
    "word2vec_output_file = path+'glove.6B.300d.txt.word2vec'\n",
    "\n",
    "# word2vec 형식으로 바꾸어 저장\n",
    "glove2word2vec(glove_input_file, word2vec_output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "\n",
    "# load the Stanford GloVe model\n",
    "filename = path+'glove.6B.300d.txt.word2vec'\n",
    "model2 = KeyedVectors.load_word2vec_format(filename, binary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate: (king - man) + woman = ?\n",
    "result = model2.most_similar(positive=['woman', 'king'], negative=['man'], topn=1)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate: (Paris - France) + Korea = ?\n",
    "result2 = model2.most_similar(positive=['paris', 'korea'], negative=['france'], topn=1)\n",
    "print(result2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FastText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 구글에서 개발한 Word2Vec을 기본으로 하되 부분단어들을 임베딩하는 기법\n",
    "# 노이즈가 많은 데이터에 유리\n",
    "\n",
    "from gensim.models import FastText\n",
    "sentences = [[\"cat\", \"say\", \"meow\"], [\"dog\", \"say\", \"woof\"]]\n",
    "\n",
    "# initialize and train\n",
    "model = FastText(sentences, min_count=1)\n",
    "model.train(sentences, total_examples=model.corpus_count, epochs=model.iter)\n",
    "say_vector = model['say']  # get vector for word\n",
    "\n",
    "print(say_vector.shape)\n",
    "print(say_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델을 학습할 때 사용한 코퍼스에 없는 단어라도\n",
    "# FastText는 그 단어를 구성하기 위해 n-grams를 덧붙여서 벡터를 만들 수 있음\n",
    "\n",
    "of_vector = model['of']  # get vector for out-of-vocab word\n",
    "\n",
    "print(of_vector.shape)\n",
    "print(of_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a model and update vocab for online training\n",
    "\n",
    "sentences_1 = [[\"cat\", \"say\", \"meow\"], [\"dog\", \"say\", \"woof\"]]\n",
    "sentences_2 = [[\"dude\", \"say\", \"wazzup!\"]]\n",
    "\n",
    "model = FastText(min_count=1)\n",
    "model.build_vocab(sentences_1)\n",
    "model.train(sentences_1, total_examples=model.corpus_count, epochs=model.iter)\n",
    "model.build_vocab(sentences_2, update=True)\n",
    "model.train(sentences_2, total_examples=model.corpus_count, epochs=model.iter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 한국어 데이터 처리하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install KoNLPy\n",
    "pip install konlpy\n",
    "\n",
    "# http://konlpy.org/ko/latest/install/ 참조"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 데이터 전처리: konlpy를 이용한 형태소 분석"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Komoran:[('이것', 'NP'), ('은', 'JX'), ('샘플', 'NNG'), ('문장', 'NNG'), ('이', 'VCP'), ('ㅂ니다', 'EF'), ('.', 'SF')]\n",
      "Twitter:[('이', 'Determiner'), ('것', 'Noun'), ('은', 'Josa'), ('샘플', 'Noun'), ('문장', 'Noun'), ('입니', 'Adjective'), ('다', 'Eomi'), ('.', 'Punctuation')]\n",
      "Hannanum:[('이것', 'N'), ('은', 'J'), ('샘플', 'N'), ('문장', 'N'), ('이', 'J'), ('ㅂ니다', 'E'), ('.', 'S')]\n"
     ]
    }
   ],
   "source": [
    "import konlpy\n",
    "\n",
    "from konlpy.tag import Komoran\n",
    "from konlpy.tag import Twitter\n",
    "from konlpy.tag import Hannanum\n",
    "# from konlpy.tag import Mecab\n",
    "# from konlpy.tag import Kkma\n",
    "\n",
    "# initialize taggers\n",
    "komo = Komoran()\n",
    "twit = Twitter()\n",
    "hann = Hannanum()\n",
    "\n",
    "sent = '이것은 샘플 문장입니다.'\n",
    "\n",
    "# Komoran parsing\n",
    "print('Komoran:'+str(komo.pos(sent)))\n",
    "# Twitter parsing\n",
    "print('Twitter:'+str(twit.pos(sent)))\n",
    "# Hannanum parsing\n",
    "print('Hannanum:'+str(hann.pos(sent)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Komoran:[('이것', 'NP'), ('은', 'JX'), ('샘플', 'NNG'), ('문장입니닼ㅋㅋ', 'NA')]\n",
      "Twitter:[('이', 'Determiner'), ('것', 'Noun'), ('은', 'Josa'), ('샘플', 'Noun'), ('문장', 'Noun'), ('입니', 'Adjective'), ('닼', 'Noun'), ('ㅋㅋ', 'KoreanParticle')]\n",
      "Hannanum:[('이것', 'N'), ('은', 'J'), ('샘플', 'N'), ('문장입니닼ㅋㅋ', 'N')]\n"
     ]
    }
   ],
   "source": [
    "sent2 = '이것은 샘플 문장입니닼ㅋㅋ'\n",
    "\n",
    "# Komoran parsing\n",
    "print('Komoran:'+str(komo.pos(sent2)))\n",
    "# Twitter parsing\n",
    "print('Twitter:'+str(twit.pos(sent2)))\n",
    "# Hannanum parsing\n",
    "print('Hannanum:'+str(hann.pos(sent2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Word2Vec 모델 학습시키기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델에 들어갈 training data 만들기\n",
    "\n",
    "# 파일 읽어 오기\n",
    "path = '/Users/sana/Documents/'\n",
    "filename = path + 'KorNewsSample.txt'\n",
    "f = open(filename, 'rU')\n",
    "lines = f.readlines()\n",
    "print(lines[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tagger = konlpy.tag.Komoran()\n",
    "\n",
    "# konlpy로 형태소 분석하기\n",
    "sentences = []\n",
    "for line in lines:\n",
    "    sentence = []\n",
    "    pos = tagger.pos(line.strip())  # list of (형태소, 품사)\n",
    "    for pair in pos:\n",
    "        morpheme = pair[0]\n",
    "        sentence.append(morpheme)\n",
    "    sentences.append(sentence)\n",
    "    \n",
    "print(len(sentences))    \n",
    "print(sentences[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "# 모델 학습시키기\n",
    "model_ko = Word2Vec(sentences, min_count=1, size=300) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습된 모델 확인하기\n",
    "print(model_ko)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델에 구축된 vocabulary 확인하기\n",
    "words = list(model_ko.wv.vocab)\n",
    "print(len(words))\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 단어 하나에 대한 임베딩 벡터 얻기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model_ko['국민'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model_ko['는'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model_ko['데이터'])  # unseen words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 별도의 파일로 모델 저장하기\n",
    "model_ko.save('model_korean.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 저장해 둔 모델 불러오기\n",
    "new_model = Word2Vec.load('model_korean.bin')\n",
    "print(new_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### model similarity test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (\"Similarity Test {한국/NNP+도쿄/NNP-서울/NNP}\")\n",
    "similars = model_ko.most_similar(positive=[u\"한국\", u\"도쿄\"], negative=[u\"서울\"], topn=10)\n",
    "for word, value in similars:\n",
    "    print(word, value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (\"Similarity Test {삼성}\")\n",
    "similars = model_ko.most_similar(positive=[u\"삼성\"], topn=10)\n",
    "for word, value in similars:\n",
    "    print(word, value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (\"Similarity Test {미래-가}\")\n",
    "similars = model_ko.most_similar(positive=[u\"미래가\"], topn=10)\n",
    "for word, value in similars:\n",
    "    print(word, value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (\"Similarity Test {미래}\")\n",
    "similars = model_ko.most_similar(positive=[u\"미래\"], topn=10)\n",
    "for word, value in similars:\n",
    "    print(word, value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 품사 정보를 포함시키는 경우"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tagger = konlpy.tag.Komoran()\n",
    "# koNLPy로 형태소 분석하기\n",
    "sentences = []\n",
    "for line in lines:\n",
    "    sentence = []\n",
    "    pos = tagger.pos(line.strip())  # list of (형태소, 품사)\n",
    "    for pair in pos:\n",
    "        morpheme = pair[0]+'/'+pair[1] # 형태소/품사 형태\n",
    "        sentence.append(morpheme)\n",
    "    sentences.append(sentence)\n",
    "    \n",
    "print(len(sentences))    \n",
    "print(sentences[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 학습시키기\n",
    "model_ko_2 = Word2Vec(sentences, min_count=1, size=300) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습된 모델 확인하기\n",
    "print(model_ko_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델에 구축된 vocabulary 확인하기\n",
    "words = list(model_ko_2.wv.vocab)\n",
    "print(len(words))\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 단어 하나에 대한 임베딩 벡터 얻기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model_ko_2['국민/NNG'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model_ko_2['는/JX'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model_ko_2['데이터/NNG'])  # unseen words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 별도의 파일로 모델 저장하기\n",
    "model_ko_2.save('model_korean_2.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 저장해 둔 모델 불러오기\n",
    "new_model = Word2Vec.load('model_korean_2.bin')\n",
    "print(new_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### model similarity test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (\"Similarity Test {한국/NNP+도쿄/NNP-서울/NNP}\")\n",
    "similars = model_ko_2.most_similar(positive=[u\"한국/NNP\", u\"도쿄/NNP\"], negative=[u\"서울/NNP\"], topn=10)\n",
    "for word, value in similars:\n",
    "    print(word, value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (\"Similarity Test {삼성/NNP}\")\n",
    "similars = model_ko_2.most_similar(positive=[u\"삼성/NNP\"], topn=10)\n",
    "for word, value in similars:\n",
    "    print(word, value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (\"Similarity Test {미래/NNG}\")\n",
    "similars = model_ko_2.most_similar(positive=[u\"미래/NNG\"], topn=10)\n",
    "for word, value in similars:\n",
    "    print(word, value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 품사 정보를 이용하여 content word 필터링하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tagger = konlpy.tag.Komoran()\n",
    "# koNLPy로 형태소 분석하기\n",
    "sentences = []\n",
    "for line in lines:\n",
    "    sentence = []\n",
    "    pos = tagger.pos(line.strip())  # list of (형태소, 품사)\n",
    "    for pair in pos:\n",
    "        if pair[1] in ['NNG', 'NNP', 'VV', 'VA']: # part-of-speech of content word\n",
    "            morpheme = pair[0]+'/'+pair[1] # 형태소/품사 형태\n",
    "            sentence.append(morpheme)\n",
    "    sentences.append(sentence)\n",
    "    \n",
    "print(len(sentences))    \n",
    "print(sentences[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 학습시키기\n",
    "model_ko_3 = Word2Vec(sentences, min_count=1, size=300) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습된 모델 확인하기\n",
    "print(model_ko_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델에 구축된 vocabulary 확인하기\n",
    "words = list(model_ko_3.wv.vocab)\n",
    "print(len(words))\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 단어 하나에 대한 임베딩 벡터 얻기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model_ko_3['국민/NNG'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model_ko_3['는/JX'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 별도의 파일로 모델 저장하기\n",
    "model_ko_2.save('model_korean_3.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 저장해 둔 모델 불러오기\n",
    "new_model = Word2Vec.load('model_korean_3.bin')\n",
    "print(new_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### model similarity test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (\"Similarity Test {한국/NNP+도쿄/NNP-서울/NNP}\")\n",
    "similars = model_ko_3.most_similar(positive=[u\"한국/NNP\", u\"도쿄/NNP\"], negative=[u\"서울/NNP\"], topn=10)\n",
    "for word, value in similars:\n",
    "    print(word, value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (\"Similarity Test {삼성/NNP}\")\n",
    "similars = model_ko_3.most_similar(positive=[u\"삼성/NNP\"], topn=10)\n",
    "for word, value in similars:\n",
    "    print(word, value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (\"Similarity Test {미래/NNG}\")\n",
    "similars = model_ko_3.most_similar(positive=[u\"미래/NNG\"], topn=10)\n",
    "for word, value in similars:\n",
    "    print(word, value)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
