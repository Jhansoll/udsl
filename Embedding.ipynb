{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install gensim\n",
    "#pip install glove\n",
    "#pip install fasttext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-05-02 14:43:20,397 : INFO : collecting all words and their counts\n",
      "2018-05-02 14:43:20,405 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2018-05-02 14:43:20,469 : INFO : PROGRESS: at sentence #10000, processed 115560 words, keeping 22375 word types\n",
      "2018-05-02 14:43:20,512 : INFO : PROGRESS: at sentence #20000, processed 234719 words, keeping 37522 word types\n",
      "2018-05-02 14:43:20,569 : INFO : PROGRESS: at sentence #30000, processed 355069 words, keeping 50030 word types\n",
      "2018-05-02 14:43:20,611 : INFO : PROGRESS: at sentence #40000, processed 476808 words, keeping 60063 word types\n",
      "2018-05-02 14:43:20,651 : INFO : PROGRESS: at sentence #50000, processed 597708 words, keeping 68854 word types\n",
      "2018-05-02 14:43:20,689 : INFO : PROGRESS: at sentence #60000, processed 714368 words, keeping 76101 word types\n",
      "2018-05-02 14:43:20,734 : INFO : PROGRESS: at sentence #70000, processed 832379 words, keeping 83203 word types\n",
      "2018-05-02 14:43:20,781 : INFO : PROGRESS: at sentence #80000, processed 958793 words, keeping 91319 word types\n",
      "2018-05-02 14:43:20,827 : INFO : PROGRESS: at sentence #90000, processed 1084845 words, keeping 98589 word types\n",
      "2018-05-02 14:43:20,837 : INFO : collected 100237 word types from a corpus of 1106744 raw words and 91804 sentences\n",
      "2018-05-02 14:43:20,838 : INFO : Loading a fresh vocabulary\n",
      "2018-05-02 14:43:20,890 : INFO : min_count=15 retains 6429 unique words (6% of original 100237, drops 93808)\n",
      "2018-05-02 14:43:20,891 : INFO : min_count=15 leaves 898335 word corpus (81% of original 1106744, drops 208409)\n",
      "2018-05-02 14:43:20,911 : INFO : deleting the raw counts dictionary of 100237 items\n",
      "2018-05-02 14:43:20,919 : INFO : sample=0.001 downsamples 43 most-common words\n",
      "2018-05-02 14:43:20,920 : INFO : downsampling leaves estimated 595310 word corpus (66.3% of prior 898335)\n",
      "2018-05-02 14:43:20,928 : INFO : constructing a huffman tree from 6429 words\n",
      "2018-05-02 14:43:21,213 : INFO : built huffman tree with maximum node depth 16\n",
      "2018-05-02 14:43:21,225 : INFO : estimated required memory for 6429 words and 300 dimensions: 27644700 bytes\n",
      "2018-05-02 14:43:21,226 : INFO : resetting layer weights\n",
      "/usr/local/lib/python3.6/site-packages/ipykernel_launcher.py:28: DeprecationWarning: Call to deprecated `iter` (Attribute will be removed in 4.0.0, use self.epochs instead).\n",
      "2018-05-02 14:43:21,331 : INFO : training model with 4 workers on 6429 vocabulary and 300 features, using sg=0 hs=1 sample=0.001 negative=5 window=5\n",
      "2018-05-02 14:43:22,334 : INFO : EPOCH 1 - PROGRESS: at 80.29% examples, 467055 words/s, in_qsize 8, out_qsize 0\n",
      "2018-05-02 14:43:22,561 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-05-02 14:43:22,574 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-05-02 14:43:22,580 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-05-02 14:43:22,586 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-05-02 14:43:22,587 : INFO : EPOCH - 1 : training on 1106744 raw words (595133 effective words) took 1.3s, 474984 effective words/s\n",
      "2018-05-02 14:43:23,595 : INFO : EPOCH 2 - PROGRESS: at 71.36% examples, 413943 words/s, in_qsize 7, out_qsize 0\n",
      "2018-05-02 14:43:23,909 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-05-02 14:43:23,916 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-05-02 14:43:23,925 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-05-02 14:43:23,926 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-05-02 14:43:23,927 : INFO : EPOCH - 2 : training on 1106744 raw words (595862 effective words) took 1.3s, 446678 effective words/s\n",
      "2018-05-02 14:43:24,931 : INFO : EPOCH 3 - PROGRESS: at 72.26% examples, 419512 words/s, in_qsize 7, out_qsize 0\n",
      "2018-05-02 14:43:25,233 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-05-02 14:43:25,240 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-05-02 14:43:25,256 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-05-02 14:43:25,257 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-05-02 14:43:25,258 : INFO : EPOCH - 3 : training on 1106744 raw words (595367 effective words) took 1.3s, 448365 effective words/s\n",
      "2018-05-02 14:43:26,264 : INFO : EPOCH 4 - PROGRESS: at 71.36% examples, 413551 words/s, in_qsize 7, out_qsize 0\n",
      "2018-05-02 14:43:26,631 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-05-02 14:43:26,633 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-05-02 14:43:26,644 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-05-02 14:43:26,645 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-05-02 14:43:26,645 : INFO : EPOCH - 4 : training on 1106744 raw words (595582 effective words) took 1.4s, 430291 effective words/s\n",
      "2018-05-02 14:43:27,657 : INFO : EPOCH 5 - PROGRESS: at 80.29% examples, 463169 words/s, in_qsize 7, out_qsize 0\n",
      "2018-05-02 14:43:27,939 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-05-02 14:43:27,954 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-05-02 14:43:27,960 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-05-02 14:43:27,968 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-05-02 14:43:27,969 : INFO : EPOCH - 5 : training on 1106744 raw words (595205 effective words) took 1.3s, 450544 effective words/s\n",
      "2018-05-02 14:43:27,971 : INFO : training on a 5533720 raw words (2977149 effective words) took 6.6s, 448458 effective words/s\n",
      "2018-05-02 14:43:27,972 : INFO : saving Word2Vec object under BrownW2vec, separately None\n",
      "2018-05-02 14:43:27,973 : INFO : not storing attribute vectors_norm\n",
      "2018-05-02 14:43:27,974 : INFO : not storing attribute cum_table\n",
      "2018-05-02 14:43:28,288 : INFO : saved BrownW2vec\n"
     ]
    }
   ],
   "source": [
    "#Brwon Corpus W2vec Modeling\n",
    "\n",
    "import gensim, logging\n",
    "import multiprocessing\n",
    "\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "\n",
    "#w2vec training parameter setting\n",
    "BrownW2vec = gensim.models.Word2Vec(size=300, window=5, min_count=15, workers=4, batch_words=10000, hs=1, negative=5)\n",
    "\n",
    "class SentenceReader:\n",
    "\n",
    "    def __init__(self, filepath):\n",
    "        self.filepath = filepath\n",
    "\n",
    "\n",
    "    def __iter__(self):\n",
    "        for line in open(self.filepath):\n",
    "            yield line.split(' ')\n",
    "\n",
    "# 사전과 학습을 형태분석된 파일을 가지고 행함\n",
    "sentences_vocab = SentenceReader('BrownAll.txt')\n",
    "sentences_train = SentenceReader('BrownAll.txt')\n",
    "\n",
    "\n",
    "BrownW2vec.build_vocab(sentences_vocab)\n",
    "#in new gensim, training has been changed. We must specify corpus count...\n",
    "BrownW2vec.train(sentences_train, total_examples=BrownW2vec.corpus_count, epochs=BrownW2vec.iter)\n",
    "#이렇게 학습된 모델을 저장\n",
    "BrownW2vec.save('BrownW2vec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "이 모델의 통계-word\n",
      "Word2Vec(vocab=6429, size=300, alpha=0.025)\n",
      "\n",
      "\n",
      "Similarity Test {France, Paris, Italy}\n",
      "Texas-0.7124084830284119 serum-0.6828513145446777 native-0.6777008771896362 River,-0.6547131538391113 Office-0.6420746445655823 Richard-0.6413450837135315 Art-0.6391761898994446 president-0.638580858707428 Pennsylvania-0.6385713815689087 assistant-0.6370608806610107\n",
      "\n",
      "\n",
      "Similarity Test {king}\n",
      "interview-0.8482460975646973 commission-0.8095151782035828 Oriental-0.7956892251968384 Ford-0.7945824265480042 Irish-0.7931922078132629 buying-0.791757345199585 exhibition-0.7845189571380615 Social-0.7825729846954346 overseas-0.7783781290054321 announcement-0.7755504846572876\n",
      "\n",
      "\n",
      "Similarity Test {president}\n",
      "E&-0.7486906051635742 K&-0.7465528249740601 version-0.7441800236701965 L&-0.7428076863288879 Richard-0.7357994318008423 B&-0.7342018485069275 M&-0.726833701133728 Director-0.7264324426651001 Council-0.7236030697822571 amateur-0.722406268119812\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/ipykernel_launcher.py:15: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n",
      "  from ipykernel import kernelapp as app\n",
      "/usr/local/lib/python3.6/site-packages/ipykernel_launcher.py:22: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n",
      "/usr/local/lib/python3.6/site-packages/ipykernel_launcher.py:29: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "model = gensim.models.Word2Vec.load('BrownW2vec')\n",
    "#modelNews.init_sims(replace = True)\n",
    "\n",
    "\n",
    "print (\"이 모델의 통계-word\")\n",
    "# no.of vocab.. in this model\n",
    "print (model)\n",
    "print (\"\\n\")\n",
    "\n",
    "#most similar Test\n",
    "\n",
    "print (\"Similarity Test {France, Paris, Italy}\")\n",
    "print (' '.join([\"{}-{}\".format(word, value) for word, value in\n",
    "(model.most_similar(positive=[u\"France\", u\"German\"], negative=[u\"Paris\"], topn=10))]))\n",
    "\n",
    "print (\"\\n\")\n",
    "\n",
    "\n",
    "print (\"Similarity Test {king}\")\n",
    "print (' '.join([\"{}-{}\".format(word, value) for word, value in\n",
    "(model.most_similar(positive=[u\"king\"], topn=10))]))\n",
    "\n",
    "print (\"\\n\")\n",
    "\n",
    "\n",
    "print (\"Similarity Test {president}\")\n",
    "print (' '.join([\"{}-{}\".format(word, value) for word, value in\n",
    "(model.most_similar(positive=[u\"president\"], topn=10))]))\n",
    "\n",
    "print (\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus built.\n",
      "Elapsed time: 3 seconds\n",
      "Dict size: 100236\n",
      "Collocations: 4808058\n",
      "Model file modelBrwon_glove_300d_5i.bin generated.\n",
      "Elapsed time: 82 seconds\n",
      "glove model embedding complete. Saved in Word2Vec format.\n",
      "Elapsed time: 82 seconds\n"
     ]
    }
   ],
   "source": [
    "#Glove Embedding\n",
    "from glove import Corpus, Glove\n",
    "import itertools\n",
    "from gensim.models.word2vec import Text8Corpus\n",
    "#import glove.corpus\n",
    "import glove\n",
    "from gensim.models import KeyedVectors\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.scripts import glove2word2vec\n",
    "import time\n",
    "\n",
    "start_time = time.time()\n",
    "def glovesave(model, filename):\n",
    "    with open(filename, mode='w', encoding='utf-8') as f:\n",
    "        for i in range(len(model.inverse_dictionary)):\n",
    "            f.write(model.inverse_dictionary[i]+' ')\n",
    "            for j in range(len(model.word_vectors[i])):\n",
    "                f.write(str(model.word_vectors[i][j])+' ')\n",
    "            f.write('\\n')\n",
    "\n",
    "corpus_model = Corpus()\n",
    "#text = ZipSentenceRead(morphZipName)\n",
    "text = list(itertools.islice(Text8Corpus('BrownAll.txt'), None))\n",
    "corpus_model.fit(text)   # window=10\n",
    "print('Corpus built.\\n'\n",
    "      'Elapsed time: %d seconds' % (time.time()-start_time))\n",
    "print('Dict size: %s' % len(corpus_model.dictionary))\n",
    "print('Collocations: %s' % corpus_model.matrix.nnz)\n",
    "#for sizes in EMBED_SIZE:\n",
    "for sizes in [300]:\n",
    "    #for iters in ITERATION:\n",
    "    for iters in [5]:\n",
    "        filename = 'modelBrwon_glove_'+str(sizes)+'d_'+str(iters)+'i'\n",
    "        model = Glove(no_components=sizes)\n",
    "        model.fit(corpus_model.matrix, epochs=iters, no_threads=4)\n",
    "        model.add_dictionary(corpus_model.dictionary)\n",
    "        #model.save(filename+'.model')\n",
    "        glovesave(model, filename+'.tmp')\n",
    "        glove2word2vec.glove2word2vec(filename+'.tmp', filename+'.txt')\n",
    "        #os.remove(filename+'.model')\n",
    "        print('Model file %s generated.' % (filename+'.bin'))\n",
    "        print('Elapsed time: %d seconds' % (time.time()-start_time))\n",
    "\n",
    "del corpus_model\n",
    "del model\n",
    "print('glove model embedding complete. Saved in Word2Vec format.\\n'\n",
    "      'Elapsed time: %d seconds' % (time.time()-start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fasttext\n",
    "\n",
    "model = fasttext.skipgram('BrownAll.txt', 'BrownFastText_model', lr=0.1, dim=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "이 모델의 통계-word\n",
      "<fasttext.model.WordVectorModel object at 0x10d8b6a90>\n",
      "\n",
      "\n",
      "Similarity Test {France, Paris, Italy}\n",
      "Dance-0.6022256016731262 France's-0.5845974683761597 Francis-0.5828438401222229 Paris,-0.5813987851142883 Francesca-0.5683878064155579 vengeance-0.5662281513214111 appearance-0.5613657236099243 Francisco-0.5430128574371338 Francisco,-0.5402178764343262 France.-0.5395409464836121\n",
      "\n",
      "\n",
      "Similarity Test {king}\n",
      "waking-0.8925457000732422 Taking-0.8910366296768188 liking-0.8889200091362 Looking-0.847740650177002 smoking-0.8447213172912598 wrecking-0.8440389037132263 winking-0.8329351544380188 sinking-0.8325021266937256 kicking-0.8301736116409302 cooking-0.8267040848731995\n",
      "\n",
      "\n",
      "Similarity Test {president}\n",
      "vice-president-0.9608277678489685 president,-0.9567978978157043 president's-0.9449669122695923 resident-0.9340541958808899 president.-0.9324999451637268 presidents-0.9207680821418762 President-0.8981149792671204 President,-0.8879393339157104 presidential-0.880010187625885 President's-0.8777850270271301\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#fasttext test\n",
    "\n",
    "from gensim.models import KeyedVectors\n",
    "model1 = KeyedVectors.load_word2vec_format('BrownFastText_model.vec')\n",
    "print (\"이 모델의 통계-word\")\n",
    "# no.of vocab.. in this model\n",
    "print (model)\n",
    "print (\"\\n\")\n",
    "\n",
    "#most similar Test\n",
    "\n",
    "print (\"Similarity Test {France, Paris, Italy}\")\n",
    "print (' '.join([\"{}-{}\".format(word, value) for word, value in\n",
    "(model1.most_similar(positive=[u\"France\", u\"Paris\"], negative=[u\"German\"], topn=10))]))\n",
    "\n",
    "print (\"\\n\")\n",
    "\n",
    "\n",
    "print (\"Similarity Test {king}\")\n",
    "print (' '.join([\"{}-{}\".format(word, value) for word, value in\n",
    "(model1.most_similar(positive=[u\"king\"], topn=10))]))\n",
    "\n",
    "print (\"\\n\")\n",
    "\n",
    "\n",
    "print (\"Similarity Test {president}\")\n",
    "print (' '.join([\"{}-{}\".format(word, value) for word, value in\n",
    "(model1.most_similar(positive=[u\"president\"], topn=10))]))\n",
    "\n",
    "print (\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#glove test\n",
    "\n",
    "import gensim\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "#model = KeyedVectors.load_word2vec_format(\"modelRaw_glove_300d_5i.txt\")\n",
    "\n",
    "model2 = KeyedVectors.load_word2vec_format('modelBrwon_glove_300d_5i.txt')\n",
    "print (\"이 모델의 통계-word\")\n",
    "# no.of vocab.. in this model\n",
    "print (model2)\n",
    "print (\"\\n\")\n",
    "\n",
    "#most similar Test\n",
    "\n",
    "print (\"Similarity Test {France, Paris, Italy}\")\n",
    "print (' '.join([\"{}-{}\".format(word, value) for word, value in\n",
    "(model2.most_similar(positive=[u\"France\", u\"Paris\"], negative=[u\"German\"], topn=10))]))\n",
    "\n",
    "print (\"\\n\")\n",
    "\n",
    "\n",
    "print (\"Similarity Test {king}\")\n",
    "print (' '.join([\"{}-{}\".format(word, value) for word, value in\n",
    "(model2.most_similar(positive=[u\"king\"], topn=10))]))\n",
    "\n",
    "print (\"\\n\")\n",
    "\n",
    "\n",
    "print (\"Similarity Test {president}\")\n",
    "print (' '.join([\"{}-{}\".format(word, value) for word, value in\n",
    "(model2.most_similar(positive=[u\"president\"], topn=10))]))\n",
    "\n",
    "print (\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
